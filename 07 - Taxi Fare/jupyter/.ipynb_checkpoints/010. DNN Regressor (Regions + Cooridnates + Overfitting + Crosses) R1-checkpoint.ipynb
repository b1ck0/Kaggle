{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(TRAIN_PATH):\n",
    "    chunksize = 5_000_000 # batch size\n",
    "    \n",
    "    traintypes = {\n",
    "        'fare': 'float32',\n",
    "        'from': 'str',\n",
    "        'to': 'str',\n",
    "        'p_count': 'uint8', \n",
    "        'hour': 'uint8',\n",
    "        'weekday': 'uint8',\n",
    "        'week': 'uint8',\n",
    "        'year': 'int16',\n",
    "        'dist_e': 'float32',\n",
    "        'dist_t': 'float32',\n",
    "        'x0': 'float32',\n",
    "        'x1': 'float32',\n",
    "        'y0': 'float32',\n",
    "        'y1': 'float32',\n",
    "    }\n",
    "    \n",
    "    cols = list(traintypes.keys())\n",
    "    \n",
    "    # loading the dataframe into list of small dataframes\n",
    "    df_list = []\n",
    "\n",
    "    for df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize):\n",
    "        df_list.append(df_chunk) \n",
    "        df = pd.concat(df_list)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_csv('../input/train_cleaned_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['fare']\n",
    "X = train.drop(['fare'], axis=1)\n",
    "del train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\n",
    "    'Manhattan', 'JFK', 'LaGardia', 'Newark', 'Teterboro', 'Brooklin', 'Bronx', 'Staten', 'Queens', 'Westchester',\n",
    "    'Nassau', 'Jersey', 'Bayonne', 'WStaton', 'West', 'WNewYork', 'North'        \n",
    "]\n",
    "\n",
    "# num_buckets = [0, num_buckets)\n",
    "\n",
    "p_count = tf.feature_column.numeric_column('p_count')\n",
    "taxicab = tf.feature_column.numeric_column('dist_t')\n",
    "euclidean = tf.feature_column.numeric_column('dist_e')\n",
    "\n",
    "x0 = tf.feature_column.numeric_column('x0')\n",
    "x1 = tf.feature_column.numeric_column('x1')\n",
    "y0 = tf.feature_column.numeric_column('y0')\n",
    "y1 = tf.feature_column.numeric_column('y1')\n",
    "\n",
    "numbuckets = 35\n",
    "dx = np.linspace(-74.27, -72.98, numbuckets).tolist()\n",
    "dy = np.linspace(40.56, 41.71, numbuckets).tolist()\n",
    "\n",
    "x0_B = tf.feature_column.bucketized_column(x0, dx)\n",
    "x1_B = tf.feature_column.bucketized_column(x1, dx)\n",
    "y0_B = tf.feature_column.bucketized_column(y0, dy)\n",
    "y1_B = tf.feature_column.bucketized_column(y1, dy)\n",
    "\n",
    "xy_0 = tf.feature_column.crossed_column([x0_B, y0_B], numbuckets * numbuckets)\n",
    "xy_1 = tf.feature_column.crossed_column([x1_B, y1_B], numbuckets * numbuckets)\n",
    "xy_pair = tf.feature_column.crossed_column([xy_0, xy_1], numbuckets**4)\n",
    "\n",
    "p_count_b = tf.feature_column.bucketized_column(p_count, [1,2,3])\n",
    "\n",
    "# definition of the categorical columns\n",
    "hour = tf.feature_column.categorical_column_with_identity('hour', num_buckets = 24)\n",
    "weekday = tf.feature_column.categorical_column_with_identity('weekday', num_buckets = 7)\n",
    "week = tf.feature_column.categorical_column_with_identity('week', num_buckets = 54)\n",
    "year = tf.feature_column.categorical_column_with_vocabulary_list('year', [2009, 2010, 2011, 2012, 2013, 2014, 2015])\n",
    "\n",
    "loc_from = tf.feature_column.categorical_column_with_vocabulary_list('from', locations)\n",
    "loc_to = tf.feature_column.categorical_column_with_vocabulary_list('to', locations)\n",
    "\n",
    "# definition of the feature crosses\n",
    "from_to = tf.feature_column.crossed_column([loc_from, loc_to], len(locations)**2)\n",
    "from_to_year = tf.feature_column.crossed_column([from_to, year], len(locations)**2 * 7)\n",
    "from_to_year_week = tf.feature_column.crossed_column([from_to_year, week], len(locations)**2 * 7 * 54)\n",
    "from_to_p_count = tf.feature_column.crossed_column([from_to, p_count_b], len(locations)**2 * 5)\n",
    "\n",
    "hour_weekday = tf.feature_column.crossed_column([hour, weekday], 24 * 7)\n",
    "weekday_week_year = tf.feature_column.crossed_column([weekday, week, year], 7 * 54 * 7)\n",
    "week_year = tf.feature_column.crossed_column([week, year], 54 * 7)\n",
    "weekday_year = tf.feature_column.crossed_column([weekday, year], 7 * 7)\n",
    "hour_weekday_year = tf.feature_column.crossed_column([hour, weekday, year], 24 * 7 * 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_columns = [\n",
    "    # Embedding_column to \"group\" together ...\n",
    "    tf.feature_column.embedding_column(from_to, 50),\n",
    "    tf.feature_column.embedding_column(from_to_year, 50),\n",
    "    tf.feature_column.embedding_column(from_to_year_week, 50),\n",
    "    tf.feature_column.embedding_column(hour_weekday, 50),\n",
    "    tf.feature_column.embedding_column(weekday_week_year, 50),\n",
    "    tf.feature_column.embedding_column(week_year,50),\n",
    "    tf.feature_column.embedding_column(weekday_year,50),\n",
    "    tf.feature_column.embedding_column(hour_weekday_year, 50),\n",
    "    tf.feature_column.embedding_column(xy_pair, 50),\n",
    "    tf.feature_column.embedding_column(from_to_p_count, 50),\n",
    "    \n",
    "\n",
    "    # indicator columns\n",
    "    tf.feature_column.indicator_column(loc_from),\n",
    "    tf.feature_column.indicator_column(loc_to),\n",
    "    tf.feature_column.indicator_column(hour),\n",
    "    tf.feature_column.indicator_column(weekday),\n",
    "    tf.feature_column.indicator_column(week),\n",
    "    tf.feature_column.indicator_column(year),\n",
    "    tf.feature_column.indicator_column(xy_0),\n",
    "    tf.feature_column.indicator_column(xy_1),\n",
    "    tf.feature_column.indicator_column(p_count_b),\n",
    "\n",
    "    # numeric columns\n",
    "    taxicab, euclidean, p_count\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        num_epochs = 1,\n",
    "        shuffle = True,\n",
    "        batch_size = batch_size)\n",
    "\n",
    "def eval_input_fn(features, labels):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        num_epochs = 1,\n",
    "        shuffle = True)\n",
    "\n",
    "def pred_input_fn(features):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = features,\n",
    "        num_epochs = 1,\n",
    "        shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './DNN_Regressor_regions_coords_overfit_crosses_r1'\n",
    "\n",
    "file_writer = tf.summary.FileWriter(model_dir)\n",
    "\n",
    "estimator = tf.estimator.DNNRegressor(\n",
    "    model_dir = model_dir,\n",
    "    feature_columns = deep_columns,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.00001),\n",
    "    hidden_units = [512,256,128,64,64,32,16], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn(X_train, y_train, batch_size=512))\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = pd.read_csv('../input/test_processed.csv', index_col='key')\n",
    "\n",
    "generator = estimator.predict(input_fn=pred_input_fn(infer), \n",
    "                              checkpoint_path='./DNN_Regressor_regions_coords_overfit_crosses/model.ckpt-46459')\n",
    "predictions = [next(generator) for i in range(len(infer))]\n",
    "values = [val['predictions'].tolist()[0] for val in predictions]\n",
    "\n",
    "ids = infer.index\n",
    "submission = pd.DataFrame()\n",
    "submission['fare_amount'] = values\n",
    "submission['key'] = ids\n",
    "submission.set_index('key', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/23.dnn_regressor_rmse_3.26.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
