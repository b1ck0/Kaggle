{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the engineered datasets\n",
    "df_train = pd.read_csv('../input/train.csv') # training set after train/test split\n",
    "df_valid = pd.read_csv('../input/valid.csv') # testing set after train/test split\n",
    "df_infer = pd.read_csv('../input/infer.csv') # inference set provided in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    205102\n",
       "1       885\n",
       "2        36\n",
       "3         5\n",
       "Name: AMT_REQ_CREDIT_BUREAU_HOUR, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['AMT_REQ_CREDIT_BUREAU_HOUR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import feature_column\n",
    "\n",
    "# will be used in the model\n",
    "ext_src_1 = numeric_column('EXT_SOURCE_1')\n",
    "ext_src_2 = numeric_column('EXT_SOURCE_2')\n",
    "ext_src_3 = numeric_column('EXT_SOURCE_3')\n",
    "\n",
    "num_buckets = 50\n",
    "score_buckets = np.linspace(0,1,num_buckets)\n",
    "\n",
    "bucket_ext_src_1 = bucketized_column(ext_src_1, score_buckets)\n",
    "bucket_ext_src_2 = bucketized_column(ext_src_2, score_buckets)\n",
    "bucket_ext_src_3 = bucketized_column(ext_src_3, score_buckets)\n",
    "\n",
    "# will be used in the model\n",
    "crossed_ext_source = crossed_column([bucket_ext_src_1, bucket_ext_src_2, bucket_ext_src_3], num_buckets**3)\n",
    "embedding_ext_source = embedding_column(crossed_ext_source, 15)\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    categorical_column_with_identity('CODE_GENDER', 2),\n",
    "    categorical_column_with_identity('NAME_INCOME_TYPE', 8),\n",
    "    categorical_column_with_identity('NAME_FAMILY_STATUS', 5),\n",
    "    categorical_column_with_identity('NAME_CONTRACT_TYPE', 2),\n",
    "    categorical_column_with_identity('FLAG_OWN_CAR', 2),\n",
    "    categorical_column_with_identity('FLAG_OWN_REALTY', 2),\n",
    "    categorical_column_with_identity(key='CNT_CHILDREN', num_buckets=5),\n",
    "    categorical_column_with_identity(key='CNT_FAM_MEMBERS', num_buckets=7),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_HOUR', num_buckets=4),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_DAY', num_buckets=3),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_WEEK', num_buckets=3),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_MON', num_buckets=11),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_QRT', num_buckets=7),\n",
    "    categorical_column_with_identity(key='AMT_REQ_CREDIT_BUREAU_YEAR', num_buckets=7),\n",
    "    categorical_column_with_identity(key='OBS_30_CNT_SOCIAL_CIRCLE', num_buckets=18),\n",
    "    categorical_column_with_identity(key='OBS_60_CNT_SOCIAL_CIRCLE', num_buckets=18),\n",
    "    categorical_column_with_identity(key='DEF_30_CNT_SOCIAL_CIRCLE', num_buckets=6),\n",
    "    categorical_column_with_identity(key='DEF_60_CNT_SOCIAL_CIRCLE', num_buckets=5),\n",
    "    categorical_column_with_identity(key='AMT_INCOME_TOTAL', num_buckets=10),\n",
    "    categorical_column_with_identity(key='AMT_CREDIT', num_buckets=10),\n",
    "    categorical_column_with_identity(key='AMT_ANNUITY', num_buckets=10),\n",
    "    categorical_column_with_identity(key='AMT_GOODS_PRICE', num_buckets=10),\n",
    "    categorical_column_with_identity(key='DAYS_BIRTH', num_buckets=10),\n",
    "    categorical_column_with_identity(key='DAYS_ID_PUBLISH', num_buckets=10),\n",
    "    categorical_column_with_identity(key='DAYS_LAST_PHONE_CHANGE', num_buckets=8),\n",
    "    categorical_column_with_identity(key='DAYS_REGISTRATION', num_buckets=10),\n",
    "    categorical_column_with_identity(key='OWN_CAR_AGE', num_buckets=10),\n",
    "    categorical_column_with_identity(key='REGION_POPULATION_RELATIVE', num_buckets=10)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# initializing the LABEL and INPUT_COLUMNS variables\n",
    "LABEL = 'TARGET'\n",
    "INPUT_COLUMNS = []\n",
    "\n",
    "# adding the numeric features to the tensorflow graph\n",
    "for feature in NUMERIC_FEATURES:\n",
    "    INPUT_COLUMNS.append(tf.feature_column.numeric_column(feature))\n",
    "    \n",
    "# adding the categorical features to the tensorflow graph\n",
    "for feature in CATEGORICAL_FEATURES:\n",
    "    INPUT_COLUMNS.append(tf.feature_column.indicator_column(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an train input function which feeds the training pandas dataframe\n",
    "def input_fn_train(df, num_epochs):\n",
    "    '''\n",
    "    inputs:\n",
    "    df - training set after train/test split\n",
    "    \n",
    "    output:\n",
    "    minibatches of x,y\n",
    "    '''\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = df,\n",
    "    y = df[LABEL],\n",
    "    batch_size = 400,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an prediction input function which feeds the inference pandas dataframe\n",
    "def input_fn_eval(df, num_epochs):\n",
    "    '''\n",
    "    inputs:\n",
    "    df - testing set after train/test split\n",
    "    \n",
    "    output:\n",
    "    minibatches of x,y\n",
    "    '''\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = df,\n",
    "    y = df[LABEL],\n",
    "    batch_size = 128,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an prediction input function which feeds the inference pandas dataframe\n",
    "def input_fn_infer(df, num_epochs):\n",
    "    '''\n",
    "    inputs:\n",
    "    df - inference dataset provided in Kaggle\n",
    "    \n",
    "    output:\n",
    "    minibatches of x,None\n",
    "    '''\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = df,\n",
    "    y = None,\n",
    "    batch_size = 128,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTDIR = 'C:\\\\Log'\n",
    "\n",
    "file_writer = tf.summary.FileWriter(OUTDIR)\n",
    "\n",
    "estimator=tf.estimator.DNNClassifier(\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        hidden_units=[6,6,6,6], \n",
    "                        feature_columns=INPUT_COLUMNS, \n",
    "                        model_dir=OUTDIR,\n",
    "                        n_classes=2,\n",
    "                        optimizer=tf.train.AdamOptimizer(learning_rate=0.00001),\n",
    "                        dropout=0.2,\n",
    "                        loss_reduction='weighted_sum')\n",
    "\n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                       input_fn = input_fn_train(df_train, 5),\n",
    "                       max_steps = 1000000)\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                       input_fn = input_fn_eval(df_valid, 5),\n",
    "                       steps = None,\n",
    "                       start_delay_secs = 1, # start evaluating after N seconds\n",
    "                       throttle_secs = 10)  # evaluate every N seconds\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
